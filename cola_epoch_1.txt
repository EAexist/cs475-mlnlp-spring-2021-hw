cola_CLS

2021-05-07 17:20:25.050050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Downloading: 28.7kB [00:00, 17.4MB/s]       
Downloading: 28.7kB [00:00, 22.3MB/s]       
Downloading: 100% 377k/377k [00:00<00:00, 608kB/s]
[INFO|file_utils.py:1386] 2021-05-07 17:20:32,834 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2r06vdyx
Downloading: 100% 570/570 [00:00<00:00, 427kB/s]
[INFO|file_utils.py:1390] 2021-05-07 17:20:33,093 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|file_utils.py:1393] 2021-05-07 17:20:33,093 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:463] 2021-05-07 17:20:33,093 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-07 17:20:33,094 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "CLS",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-07 17:20:33,353 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-07 17:20:33,354 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|file_utils.py:1386] 2021-05-07 17:20:33,612 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd6p4r3gq
Downloading: 100% 213k/213k [00:00<00:00, 663kB/s]
[INFO|file_utils.py:1390] 2021-05-07 17:20:34,196 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|file_utils.py:1393] 2021-05-07 17:20:34,196 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|file_utils.py:1386] 2021-05-07 17:20:34,454 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpslb8hbpr
Downloading: 100% 436k/436k [00:00<00:00, 1.11MB/s]
[INFO|file_utils.py:1390] 2021-05-07 17:20:35,117 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|file_utils.py:1393] 2021-05-07 17:20:35,118 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|file_utils.py:1386] 2021-05-07 17:20:35,893 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf3j36rwm
Downloading: 100% 29.0/29.0 [00:00<00:00, 21.3kB/s]
[INFO|file_utils.py:1390] 2021-05-07 17:20:36,151 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1393] 2021-05-07 17:20:36,152 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|tokenization_utils_base.py:1702] 2021-05-07 17:20:36,152 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-07 17:20:36,152 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-07 17:20:36,152 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-07 17:20:36,152 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-07 17:20:36,152 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1386] 2021-05-07 17:20:36,439 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8njl64nt
Downloading: 100% 436M/436M [00:29<00:00, 14.8MB/s]
[INFO|file_utils.py:1390] 2021-05-07 17:21:06,027 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[INFO|file_utils.py:1393] 2021-05-07 17:21:06,028 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[INFO|modeling_utils.py:1051] 2021-05-07 17:21:06,028 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-07 17:21:10,528 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-07 17:21:10,528 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100% 9/9 [00:00<00:00, 15.43ba/s]
100% 2/2 [00:00<00:00, 24.55ba/s]
100% 2/2 [00:00<00:00, 25.70ba/s]
Downloading: 5.75kB [00:00, 4.36MB/s]       
[INFO|trainer.py:483] 2021-05-07 17:21:14,201 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-07 17:21:14,202 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-07 17:21:14,447 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-07 17:21:14,447 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-07 17:21:14,447 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-07 17:21:14,447 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-07 17:21:14,447 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-07 17:21:14,447 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-07 17:21:14,448 >>   Total optimization steps = 804
 62% 500/804 [09:46<05:57,  1.17s/it][INFO|trainer.py:1558] 2021-05-07 17:31:00,603 >> Saving model checkpoint to /tmp/cola/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-07 17:31:00,604 >> Configuration saved in /tmp/cola/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-07 17:31:02,206 >> Model weights saved in /tmp/cola/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-07 17:31:02,208 >> tokenizer config file saved in /tmp/cola/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-07 17:31:02,209 >> Special tokens file saved in /tmp/cola/checkpoint-500/special_tokens_map.json
100% 804/804 [15:46<00:00,  1.08it/s][INFO|trainer.py:1129] 2021-05-07 17:37:01,263 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [15:46<00:00,  1.18s/it]
[INFO|trainer.py:1558] 2021-05-07 17:37:01,679 >> Saving model checkpoint to /tmp/cola/
[INFO|configuration_utils.py:314] 2021-05-07 17:37:01,680 >> Configuration saved in /tmp/cola/config.json
[INFO|modeling_utils.py:837] 2021-05-07 17:37:03,210 >> Model weights saved in /tmp/cola/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-07 17:37:03,210 >> tokenizer config file saved in /tmp/cola/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-07 17:37:03,211 >> Special tokens file saved in /tmp/cola/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-07 17:37:03,249 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,249 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,249 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,249 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,249 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,249 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,249 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,250 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,250 >>   train_mem_gpu_alloc_delta  =   1297MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,250 >>   train_mem_gpu_peaked_delta =   3393MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,250 >>   train_runtime              = 946.8154
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,250 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:03,250 >>   train_samples_per_second   =    0.849
[INFO|trainer.py:483] 2021-05-07 17:37:03,367 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-07 17:37:03,369 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-07 17:37:03,369 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-07 17:37:03,370 >>   Batch size = 8
100% 131/131 [00:14<00:00,  9.08it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-07 17:37:17,937 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_loss                 =  0.5559
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_matthews_correlation =  0.5572
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_runtime              = 14.4559
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-07 17:37:17,938 >>   eval_samples_per_second   =  72.151

env: POOL_NAME=CLS
2021-05-08 19:48:24.676822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Downloading: 28.7kB [00:00, 10.0MB/s]       
Downloading: 28.7kB [00:00, 31.0MB/s]       
Downloading: 100% 377k/377k [00:00<00:00, 3.13MB/s]
[INFO|file_utils.py:1386] 2021-05-08 19:48:29,213 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpha1_wzs4
Downloading: 100% 570/570 [00:00<00:00, 716kB/s]
[INFO|file_utils.py:1390] 2021-05-08 19:48:29,432 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|file_utils.py:1393] 2021-05-08 19:48:29,433 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:463] 2021-05-08 19:48:29,433 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 19:48:29,434 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "CLS",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 19:48:29,643 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 19:48:29,643 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|file_utils.py:1386] 2021-05-08 19:48:29,854 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8b_um6oi
Downloading: 100% 213k/213k [00:00<00:00, 824kB/s]
[INFO|file_utils.py:1390] 2021-05-08 19:48:30,324 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|file_utils.py:1393] 2021-05-08 19:48:30,324 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|file_utils.py:1386] 2021-05-08 19:48:30,534 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpuc6lnt9x
Downloading: 100% 436k/436k [00:00<00:00, 1.28MB/s]
[INFO|file_utils.py:1390] 2021-05-08 19:48:31,094 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|file_utils.py:1393] 2021-05-08 19:48:31,095 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|file_utils.py:1386] 2021-05-08 19:48:31,733 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpe7ua784q
Downloading: 100% 29.0/29.0 [00:00<00:00, 34.5kB/s]
[INFO|file_utils.py:1390] 2021-05-08 19:48:31,945 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1393] 2021-05-08 19:48:31,945 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:48:31,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:48:31,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:48:31,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:48:31,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:48:31,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1386] 2021-05-08 19:48:32,190 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl7e3ukp8
Downloading: 100% 436M/436M [00:06<00:00, 64.9MB/s]
[INFO|file_utils.py:1390] 2021-05-08 19:48:39,194 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[INFO|file_utils.py:1393] 2021-05-08 19:48:39,194 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[INFO|modeling_utils.py:1051] 2021-05-08 19:48:39,194 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 19:48:42,497 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 19:48:42,497 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100% 9/9 [00:00<00:00, 18.34ba/s]
100% 2/2 [00:00<00:00, 32.71ba/s]
100% 2/2 [00:00<00:00, 34.31ba/s]
Downloading: 5.75kB [00:00, 6.65MB/s]       
[INFO|trainer.py:483] 2021-05-08 19:48:46,469 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 19:48:46,470 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 19:48:46,642 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 19:48:46,642 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 19:48:46,642 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 19:48:46,642 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 19:48:46,642 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 19:48:46,643 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 19:48:46,643 >>   Total optimization steps = 804
 62% 500/804 [05:37<03:28,  1.46it/s][INFO|trainer.py:1558] 2021-05-08 19:54:24,132 >> Saving model checkpoint to /tmp/cola/CLS/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 19:54:24,133 >> Configuration saved in /tmp/cola/CLS/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 19:54:25,599 >> Model weights saved in /tmp/cola/CLS/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 19:54:25,600 >> tokenizer config file saved in /tmp/cola/CLS/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 19:54:25,601 >> Special tokens file saved in /tmp/cola/CLS/checkpoint-500/special_tokens_map.json
100% 804/804 [09:10<00:00,  1.87it/s][INFO|trainer.py:1129] 2021-05-08 19:57:56,937 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [09:10<00:00,  1.46it/s]
[INFO|trainer.py:1558] 2021-05-08 19:57:57,150 >> Saving model checkpoint to /tmp/cola/CLS
[INFO|configuration_utils.py:314] 2021-05-08 19:57:57,151 >> Configuration saved in /tmp/cola/CLS/config.json
[INFO|modeling_utils.py:837] 2021-05-08 19:57:58,635 >> Model weights saved in /tmp/cola/CLS/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 19:57:58,636 >> tokenizer config file saved in /tmp/cola/CLS/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 19:57:58,636 >> Special tokens file saved in /tmp/cola/CLS/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 19:57:58,674 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,674 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,674 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,674 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,674 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,674 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,675 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,675 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,675 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,675 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,675 >>   train_runtime              = 550.2942
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,675 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:57:58,675 >>   train_samples_per_second   =    1.461
[INFO|trainer.py:483] 2021-05-08 19:57:58,810 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 19:57:58,812 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 19:57:58,812 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 19:57:58,812 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.03it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 19:58:07,651 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,651 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,651 >>   eval_loss                 =  0.5298
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,652 >>   eval_matthews_correlation =  0.5701
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,652 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,652 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,652 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,652 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,652 >>   eval_runtime              =  8.7346
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,652 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 19:58:07,652 >>   eval_samples_per_second   = 119.411




cola_MEAN_MAX

env: POOL_NAME=MEAN_MAX
2021-05-08 01:08:13.875244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Downloading: 28.7kB [00:00, 22.3MB/s]       
Downloading: 28.7kB [00:00, 24.0MB/s]       
Downloading: 100% 377k/377k [00:00<00:00, 1.15MB/s]
[INFO|file_utils.py:1386] 2021-05-08 01:08:18,829 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsnfjsraa
Downloading: 100% 570/570 [00:00<00:00, 766kB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:08:18,846 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|file_utils.py:1393] 2021-05-08 01:08:18,847 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:463] 2021-05-08 01:08:18,847 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:08:18,848 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "%POOL_NAME",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 01:08:18,863 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:08:18,863 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|file_utils.py:1386] 2021-05-08 01:08:18,880 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm5i2fx0o
Downloading: 100% 213k/213k [00:00<00:00, 13.8MB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:08:18,915 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|file_utils.py:1393] 2021-05-08 01:08:18,915 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|file_utils.py:1386] 2021-05-08 01:08:18,930 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm0he_98g
Downloading: 100% 436k/436k [00:00<00:00, 17.5MB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:08:18,977 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|file_utils.py:1393] 2021-05-08 01:08:18,977 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|file_utils.py:1386] 2021-05-08 01:08:19,027 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplgxhxii2
Downloading: 100% 29.0/29.0 [00:00<00:00, 31.8kB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:08:19,043 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1393] 2021-05-08 01:08:19,043 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:19,044 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:19,044 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:19,044 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:19,044 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:19,044 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1386] 2021-05-08 01:08:19,089 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl0284h7z
Downloading: 100% 436M/436M [00:11<00:00, 39.0MB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:08:30,369 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[INFO|file_utils.py:1393] 2021-05-08 01:08:30,369 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[INFO|modeling_utils.py:1051] 2021-05-08 01:08:30,369 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
Traceback (most recent call last):
  File "run_glue.py", line 561, in <module>
    main()
  File "run_glue.py", line 333, in main
    use_auth_token=True if model_args.use_auth_token else None,
  File "/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py", line 1058, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/content/drive/My Drive/2021 봄학기/CS475 자연언어처리를 위한 기계학습/cs475-mlnlp-spring-2021-hw/hw3/bert_poolers.py", line 147, in __init__
    self.bert = MyBertModel(config)
  File "/content/drive/My Drive/2021 봄학기/CS475 자연언어처리를 위한 기계학습/cs475-mlnlp-spring-2021-hw/hw3/bert_poolers.py", line 133, in __init__
    raise ValueError(f"Wrong pooling_layer_type: {config.pooling_layer_type}")
ValueError: Wrong pooling_layer_type: %POOL_NAME

env: POOL_NAME=MEAN_MAX
2021-05-08 19:58:10.910648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 19:58:13,151 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 19:58:13,152 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MEAN_MAX",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 19:58:13,521 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 19:58:13,522 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:58:14,591 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:58:14,591 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:58:14,591 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:58:14,591 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 19:58:14,591 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 19:58:14,825 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 19:58:18,320 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 19:58:18,320 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.linear.weight', 'bert.pooler.linear.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 19:58:22,185 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:483] 2021-05-08 19:58:22,186 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:946] 2021-05-08 19:58:22,353 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 19:58:22,353 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 19:58:22,353 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 19:58:22,353 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 19:58:22,353 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 19:58:22,353 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 19:58:22,353 >>   Total optimization steps = 804
 62% 500/804 [06:10<03:45,  1.35it/s][INFO|trainer.py:1558] 2021-05-08 20:04:32,636 >> Saving model checkpoint to /tmp/cola/MEAN_MAX/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 20:04:32,637 >> Configuration saved in /tmp/cola/MEAN_MAX/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:04:34,176 >> Model weights saved in /tmp/cola/MEAN_MAX/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:04:34,177 >> tokenizer config file saved in /tmp/cola/MEAN_MAX/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:04:34,177 >> Special tokens file saved in /tmp/cola/MEAN_MAX/checkpoint-500/special_tokens_map.json
100% 804/804 [09:59<00:00,  1.73it/s][INFO|trainer.py:1129] 2021-05-08 20:08:21,613 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [09:59<00:00,  1.34it/s]
[INFO|trainer.py:1558] 2021-05-08 20:08:21,816 >> Saving model checkpoint to /tmp/cola/MEAN_MAX
[INFO|configuration_utils.py:314] 2021-05-08 20:08:21,817 >> Configuration saved in /tmp/cola/MEAN_MAX/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:08:23,432 >> Model weights saved in /tmp/cola/MEAN_MAX/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:08:23,433 >> tokenizer config file saved in /tmp/cola/MEAN_MAX/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:08:23,433 >> Special tokens file saved in /tmp/cola/MEAN_MAX/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:08:23,480 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,480 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,480 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,480 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   init_mem_gpu_alloc_delta   =    415MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   train_mem_gpu_alloc_delta  =   1304MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   train_mem_gpu_peaked_delta =   3396MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   train_runtime              = 599.2603
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:23,481 >>   train_samples_per_second   =    1.342
[INFO|trainer.py:483] 2021-05-08 20:08:23,579 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:1775] 2021-05-08 20:08:23,581 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 20:08:23,581 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 20:08:23,581 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.09it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:08:32,732 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_loss                 =  0.5118
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_matthews_correlation =  0.5445
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_runtime              =   9.047
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:08:32,732 >>   eval_samples_per_second   = 115.287



cola_MY_STO1

env: POOL_NAME=MY_STO1
2021-05-08 01:08:33.761325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 01:08:35,374 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:08:35,374 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO1",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 01:08:35,391 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:08:35,391 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:35,470 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:35,470 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:35,470 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:35,471 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:08:35,471 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 01:08:35,511 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 01:08:38,896 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 01:08:38,896 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100% 9/9 [00:00<00:00, 20.99ba/s]
100% 2/2 [00:00<00:00, 34.25ba/s]
100% 2/2 [00:00<00:00,  9.43ba/s]
Downloading: 5.75kB [00:00, 5.82MB/s]       
[INFO|trainer.py:483] 2021-05-08 01:08:43,196 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:483] 2021-05-08 01:08:43,197 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:946] 2021-05-08 01:08:43,368 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 01:08:43,368 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 01:08:43,368 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 01:08:43,368 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 01:08:43,368 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 01:08:43,368 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 01:08:43,368 >>   Total optimization steps = 804
 62% 500/804 [06:04<03:48,  1.33it/s][INFO|trainer.py:1558] 2021-05-08 01:14:47,860 >> Saving model checkpoint to /tmp/cola/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 01:14:47,861 >> Configuration saved in /tmp/cola/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 01:14:49,439 >> Model weights saved in /tmp/cola/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 01:14:49,440 >> tokenizer config file saved in /tmp/cola/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 01:14:49,440 >> Special tokens file saved in /tmp/cola/checkpoint-500/special_tokens_map.json
100% 804/804 [09:57<00:00,  1.70it/s][INFO|trainer.py:1129] 2021-05-08 01:18:40,973 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [09:57<00:00,  1.35it/s]
[INFO|trainer.py:1558] 2021-05-08 01:18:41,171 >> Saving model checkpoint to /tmp/cola/
[INFO|configuration_utils.py:314] 2021-05-08 01:18:41,172 >> Configuration saved in /tmp/cola/config.json
[INFO|modeling_utils.py:837] 2021-05-08 01:18:42,634 >> Model weights saved in /tmp/cola/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 01:18:42,635 >> tokenizer config file saved in /tmp/cola/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 01:18:42,635 >> Special tokens file saved in /tmp/cola/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 01:18:42,663 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   train_runtime              = 597.6055
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:42,664 >>   train_samples_per_second   =    1.345
[INFO|trainer.py:483] 2021-05-08 01:18:42,767 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:1775] 2021-05-08 01:18:42,768 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 01:18:42,768 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 01:18:42,769 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.59it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 01:18:51,911 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,911 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,911 >>   eval_loss                 =  0.4982
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,911 >>   eval_matthews_correlation =  0.5701
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,911 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,911 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,911 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,911 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,911 >>   eval_runtime              =   9.039
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,912 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:18:51,912 >>   eval_samples_per_second   = 115.389

env: POOL_NAME=MY_STO1
2021-05-08 01:18:54.698120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 01:18:56,421 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:18:56,422 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO1",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 01:18:56,437 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:18:56,438 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:18:56,518 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:18:56,518 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:18:56,518 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:18:56,518 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:18:56,518 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 01:18:56,559 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 01:18:59,935 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 01:18:59,935 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 01:19:03,118 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:483] 2021-05-08 01:19:03,119 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:860] 2021-05-08 01:19:03,299 >> Loading model from /tmp/cola/checkpoint-500).
[INFO|configuration_utils.py:461] 2021-05-08 01:19:03,299 >> loading configuration file /tmp/cola/checkpoint-500/config.json
[INFO|configuration_utils.py:499] 2021-05-08 01:19:03,300 >> Model config BertConfig {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "MyBertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO1",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|modeling_utils.py:1049] 2021-05-08 01:19:03,300 >> loading weights file /tmp/cola/checkpoint-500/pytorch_model.bin
[INFO|modeling_utils.py:1167] 2021-05-08 01:19:06,453 >> All model checkpoint weights were used when initializing MyBertForSequenceClassification.

[INFO|modeling_utils.py:1176] 2021-05-08 01:19:06,453 >> All the weights of MyBertForSequenceClassification were initialized from the model checkpoint at /tmp/cola/checkpoint-500.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MyBertForSequenceClassification for predictions without further training.
[INFO|trainer.py:946] 2021-05-08 01:19:07,260 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 01:19:07,260 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 01:19:07,260 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 01:19:07,260 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 01:19:07,260 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 01:19:07,260 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 01:19:07,260 >>   Total optimization steps = 804
[INFO|trainer.py:971] 2021-05-08 01:19:07,261 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:972] 2021-05-08 01:19:07,261 >>   Continuing training from epoch 1
[INFO|trainer.py:973] 2021-05-08 01:19:07,261 >>   Continuing training from global step 500
[INFO|trainer.py:976] 2021-05-08 01:19:07,261 >>   Will skip the first 1 epochs then the first 232 batches in the first epoch.
100% 804/804 [03:51<00:00,  1.70it/s][INFO|trainer.py:1129] 2021-05-08 01:22:58,754 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [03:51<00:00,  3.47it/s]
[INFO|trainer.py:1558] 2021-05-08 01:22:58,962 >> Saving model checkpoint to /tmp/cola/
[INFO|configuration_utils.py:314] 2021-05-08 01:22:58,963 >> Configuration saved in /tmp/cola/config.json
[INFO|modeling_utils.py:837] 2021-05-08 01:23:00,647 >> Model weights saved in /tmp/cola/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 01:23:00,647 >> tokenizer config file saved in /tmp/cola/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 01:23:00,648 >> Special tokens file saved in /tmp/cola/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 01:23:00,681 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   train_mem_cpu_alloc_delta  =      1MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   train_mem_cpu_peaked_delta =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   train_mem_gpu_alloc_delta  =   1682MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   train_runtime              = 231.4938
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:00,681 >>   train_samples_per_second   =    3.473
[INFO|trainer.py:483] 2021-05-08 01:23:00,787 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:1775] 2021-05-08 01:23:00,789 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 01:23:00,789 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 01:23:00,789 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.80it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 01:23:09,833 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   eval_loss                 =  0.4812
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   eval_matthews_correlation =  0.5733
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   eval_runtime              =  8.9434
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,833 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:23:09,834 >>   eval_samples_per_second   = 116.623

env: POOL_NAME=MY_STO1
2021-05-08 01:23:23.855554: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 01:23:25,363 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:23:25,364 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO1",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 01:23:25,381 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:23:25,381 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:23:25,456 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:23:25,456 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:23:25,456 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:23:25,456 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:23:25,456 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 01:23:25,503 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 01:23:28,830 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 01:23:28,830 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 01:23:31,892 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:483] 2021-05-08 01:23:31,893 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:860] 2021-05-08 01:23:32,058 >> Loading model from /tmp/cola/checkpoint-500).
[INFO|configuration_utils.py:461] 2021-05-08 01:23:32,059 >> loading configuration file /tmp/cola/checkpoint-500/config.json
[INFO|configuration_utils.py:499] 2021-05-08 01:23:32,059 >> Model config BertConfig {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "MyBertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO1",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|modeling_utils.py:1049] 2021-05-08 01:23:32,060 >> loading weights file /tmp/cola/checkpoint-500/pytorch_model.bin
[INFO|modeling_utils.py:1167] 2021-05-08 01:23:35,157 >> All model checkpoint weights were used when initializing MyBertForSequenceClassification.

[INFO|modeling_utils.py:1176] 2021-05-08 01:23:35,158 >> All the weights of MyBertForSequenceClassification were initialized from the model checkpoint at /tmp/cola/checkpoint-500.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MyBertForSequenceClassification for predictions without further training.
[INFO|trainer.py:946] 2021-05-08 01:23:35,970 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 01:23:35,970 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 01:23:35,971 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 01:23:35,971 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 01:23:35,971 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 01:23:35,971 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 01:23:35,971 >>   Total optimization steps = 804
[INFO|trainer.py:971] 2021-05-08 01:23:35,971 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:972] 2021-05-08 01:23:35,971 >>   Continuing training from epoch 1
[INFO|trainer.py:973] 2021-05-08 01:23:35,971 >>   Continuing training from global step 500
[INFO|trainer.py:976] 2021-05-08 01:23:35,971 >>   Will skip the first 1 epochs then the first 232 batches in the first epoch.
100% 804/804 [03:50<00:00,  1.70it/s][INFO|trainer.py:1129] 2021-05-08 01:27:26,839 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [03:50<00:00,  3.48it/s]
[INFO|trainer.py:1558] 2021-05-08 01:27:27,041 >> Saving model checkpoint to /tmp/cola/
[INFO|configuration_utils.py:314] 2021-05-08 01:27:27,042 >> Configuration saved in /tmp/cola/config.json
[INFO|modeling_utils.py:837] 2021-05-08 01:27:28,282 >> Model weights saved in /tmp/cola/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 01:27:28,282 >> tokenizer config file saved in /tmp/cola/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 01:27:28,283 >> Special tokens file saved in /tmp/cola/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 01:27:28,311 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,311 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,311 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,311 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,311 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,312 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,312 >>   train_mem_cpu_alloc_delta  =      1MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,312 >>   train_mem_cpu_peaked_delta =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,312 >>   train_mem_gpu_alloc_delta  =   1682MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,312 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,312 >>   train_runtime              = 230.8688
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,312 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:28,312 >>   train_samples_per_second   =    3.482
[INFO|trainer.py:483] 2021-05-08 01:27:28,413 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:1775] 2021-05-08 01:27:28,414 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 01:27:28,414 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 01:27:28,414 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.88it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 01:27:37,368 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,368 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,368 >>   eval_loss                 =  0.4812
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,369 >>   eval_matthews_correlation =  0.5733
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,369 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,369 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,369 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,369 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,369 >>   eval_runtime              =  8.8493
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,369 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 01:27:37,369 >>   eval_samples_per_second   = 117.862

env: POOL_NAME=MY_STO1
2021-05-08 01:50:45.124525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Downloading: 28.7kB [00:00, 24.2MB/s]       
Downloading: 28.7kB [00:00, 31.4MB/s]       
Downloading: 100% 377k/377k [00:00<00:00, 1.14MB/s]
[INFO|file_utils.py:1386] 2021-05-08 01:50:49,249 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_gn1kwqm
Downloading: 100% 570/570 [00:00<00:00, 836kB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:50:49,267 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|file_utils.py:1393] 2021-05-08 01:50:49,267 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:463] 2021-05-08 01:50:49,268 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:50:49,268 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO1",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 01:50:49,283 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 01:50:49,284 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|file_utils.py:1386] 2021-05-08 01:50:49,300 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpo3670oe5
Downloading: 100% 213k/213k [00:00<00:00, 15.6MB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:50:49,331 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|file_utils.py:1393] 2021-05-08 01:50:49,332 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|file_utils.py:1386] 2021-05-08 01:50:49,349 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpiu_grl17
Downloading: 100% 436k/436k [00:00<00:00, 22.5MB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:50:49,389 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|file_utils.py:1393] 2021-05-08 01:50:49,389 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|file_utils.py:1386] 2021-05-08 01:50:49,433 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0ivw6o15
Downloading: 100% 29.0/29.0 [00:00<00:00, 44.5kB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:50:49,449 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1393] 2021-05-08 01:50:49,449 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:50:49,449 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:50:49,449 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:50:49,449 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:50:49,450 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 01:50:49,450 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1386] 2021-05-08 01:50:49,494 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7_b_8xf_
Downloading: 100% 436M/436M [00:17<00:00, 25.6MB/s]
[INFO|file_utils.py:1390] 2021-05-08 01:51:06,666 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[INFO|file_utils.py:1393] 2021-05-08 01:51:06,666 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[INFO|modeling_utils.py:1051] 2021-05-08 01:51:06,667 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 01:51:10,079 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 01:51:10,079 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
100% 9/9 [00:00<00:00, 19.29ba/s]
100% 2/2 [00:00<00:00, 30.98ba/s]
100% 2/2 [00:00<00:00, 27.89ba/s]
Downloading: 5.75kB [00:00, 6.69MB/s]       
[INFO|trainer.py:483] 2021-05-08 01:51:14,022 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 01:51:14,023 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 01:51:14,194 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 01:51:14,194 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 01:51:14,194 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 01:51:14,194 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 01:51:14,194 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 01:51:14,195 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 01:51:14,195 >>   Total optimization steps = 804
 62% 500/804 [06:12<03:49,  1.32it/s][INFO|trainer.py:1558] 2021-05-08 01:57:26,308 >> Saving model checkpoint to /tmp/cola/MY_STO1/3/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 01:57:26,309 >> Configuration saved in /tmp/cola/MY_STO1/3/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 01:57:27,868 >> Model weights saved in /tmp/cola/MY_STO1/3/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 01:57:27,870 >> tokenizer config file saved in /tmp/cola/MY_STO1/3/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 01:57:27,870 >> Special tokens file saved in /tmp/cola/MY_STO1/3/checkpoint-500/special_tokens_map.json
100% 804/804 [10:06<00:00,  1.69it/s][INFO|trainer.py:1129] 2021-05-08 02:01:20,541 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:06<00:00,  1.33it/s]
[INFO|trainer.py:1558] 2021-05-08 02:01:20,759 >> Saving model checkpoint to /tmp/cola/MY_STO1/3/
[INFO|configuration_utils.py:314] 2021-05-08 02:01:20,760 >> Configuration saved in /tmp/cola/MY_STO1/3/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:01:22,318 >> Model weights saved in /tmp/cola/MY_STO1/3/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:01:22,318 >> tokenizer config file saved in /tmp/cola/MY_STO1/3/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:01:22,318 >> Special tokens file saved in /tmp/cola/MY_STO1/3/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:01:22,352 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   train_runtime              = 606.3462
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:22,352 >>   train_samples_per_second   =    1.326
[INFO|trainer.py:483] 2021-05-08 02:01:22,455 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 02:01:22,457 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 02:01:22,457 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 02:01:22,457 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.80it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:01:31,499 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_loss                 =  0.4982
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_matthews_correlation =  0.5701
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_runtime              =  8.9381
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:01:31,499 >>   eval_samples_per_second   = 116.691

env: POOL_NAME=MY_STO1
2021-05-08 20:19:03.196708: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 20:19:05,381 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:19:05,381 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO1",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 20:19:05,736 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:19:05,736 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:19:06,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:19:06,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:19:06,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:19:06,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:19:06,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 20:19:06,985 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 20:19:10,335 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 20:19:10,335 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 20:19:13,912 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 20:19:13,912 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 20:19:14,083 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 20:19:14,083 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 20:19:14,083 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 20:19:14,083 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 20:19:14,083 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 20:19:14,083 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 20:19:14,083 >>   Total optimization steps = 804
 62% 500/804 [06:10<03:45,  1.35it/s][INFO|trainer.py:1558] 2021-05-08 20:25:24,313 >> Saving model checkpoint to /tmp/cola/MY_STO1/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 20:25:24,314 >> Configuration saved in /tmp/cola/MY_STO1/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:25:25,904 >> Model weights saved in /tmp/cola/MY_STO1/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:25:25,905 >> tokenizer config file saved in /tmp/cola/MY_STO1/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:25:25,905 >> Special tokens file saved in /tmp/cola/MY_STO1/checkpoint-500/special_tokens_map.json
100% 804/804 [09:59<00:00,  1.72it/s][INFO|trainer.py:1129] 2021-05-08 20:29:13,542 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [09:59<00:00,  1.34it/s]
[INFO|trainer.py:1558] 2021-05-08 20:29:13,748 >> Saving model checkpoint to /tmp/cola/MY_STO1
[INFO|configuration_utils.py:314] 2021-05-08 20:29:13,748 >> Configuration saved in /tmp/cola/MY_STO1/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:29:15,354 >> Model weights saved in /tmp/cola/MY_STO1/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:29:15,354 >> tokenizer config file saved in /tmp/cola/MY_STO1/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:29:15,355 >> Special tokens file saved in /tmp/cola/MY_STO1/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:29:15,387 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   train_runtime              = 599.4594
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:15,388 >>   train_samples_per_second   =    1.341
[INFO|trainer.py:483] 2021-05-08 20:29:15,492 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 20:29:15,494 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 20:29:15,494 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 20:29:15,494 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.14it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:29:24,296 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,296 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,296 >>   eval_loss                 =  0.4982
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,296 >>   eval_matthews_correlation =  0.5701
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,296 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,297 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,297 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,297 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,297 >>   eval_runtime              =  8.6968
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,297 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:29:24,297 >>   eval_samples_per_second   = 119.929




cola MY_STO2_1

env: POOL_NAME=MY_STO2
2021-05-08 20:29:26.772600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 20:29:29,172 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:29:29,172 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO2",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 20:29:29,375 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:29:29,375 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:29:30,424 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:29:30,424 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:29:30,424 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:29:30,424 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:29:30,425 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 20:29:30,652 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 20:29:34,086 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 20:29:34,087 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 20:29:37,662 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 20:29:37,663 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 20:29:37,834 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 20:29:37,834 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 20:29:37,834 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 20:29:37,834 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 20:29:37,834 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 20:29:37,834 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 20:29:37,834 >>   Total optimization steps = 804
 62% 500/804 [06:10<03:45,  1.35it/s][INFO|trainer.py:1558] 2021-05-08 20:35:48,366 >> Saving model checkpoint to /tmp/cola/MY_STO2/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 20:35:48,367 >> Configuration saved in /tmp/cola/MY_STO2/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:35:49,781 >> Model weights saved in /tmp/cola/MY_STO2/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:35:49,782 >> tokenizer config file saved in /tmp/cola/MY_STO2/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:35:49,782 >> Special tokens file saved in /tmp/cola/MY_STO2/checkpoint-500/special_tokens_map.json
100% 804/804 [09:59<00:00,  1.73it/s][INFO|trainer.py:1129] 2021-05-08 20:39:37,259 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [09:59<00:00,  1.34it/s]
[INFO|trainer.py:1558] 2021-05-08 20:39:37,465 >> Saving model checkpoint to /tmp/cola/MY_STO2
[INFO|configuration_utils.py:314] 2021-05-08 20:39:37,466 >> Configuration saved in /tmp/cola/MY_STO2/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:39:39,063 >> Model weights saved in /tmp/cola/MY_STO2/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:39:39,064 >> tokenizer config file saved in /tmp/cola/MY_STO2/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:39:39,064 >> Special tokens file saved in /tmp/cola/MY_STO2/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:39:39,095 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,095 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,095 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,095 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,095 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,096 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,096 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,096 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,096 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,096 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,096 >>   train_runtime              = 599.4247
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,096 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:39,096 >>   train_samples_per_second   =    1.341
[INFO|trainer.py:483] 2021-05-08 20:39:39,206 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 20:39:39,208 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 20:39:39,208 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 20:39:39,208 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.21it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:39:47,976 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_loss                 =  0.5011
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_matthews_correlation =  0.5829
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_runtime              =  8.6602
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:39:47,976 >>   eval_samples_per_second   = 120.435

env: POOL_NAME=MY_STO2
2021-05-08 02:01:34.743743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 02:01:36,528 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:01:36,528 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO2",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 02:01:36,546 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:01:36,546 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:01:36,626 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:01:36,626 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:01:36,626 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:01:36,626 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:01:36,626 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 02:01:36,676 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 02:01:40,170 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 02:01:40,171 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 02:01:43,386 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 02:01:43,387 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 02:01:43,572 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 02:01:43,572 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 02:01:43,572 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 02:01:43,572 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 02:01:43,572 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 02:01:43,572 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 02:01:43,572 >>   Total optimization steps = 804
 62% 500/804 [06:18<03:50,  1.32it/s][INFO|trainer.py:1558] 2021-05-08 02:08:02,163 >> Saving model checkpoint to /tmp/cola/MY_STO2/1/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 02:08:02,164 >> Configuration saved in /tmp/cola/MY_STO2/1/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:08:03,743 >> Model weights saved in /tmp/cola/MY_STO2/1/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:08:03,744 >> tokenizer config file saved in /tmp/cola/MY_STO2/1/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:08:03,744 >> Special tokens file saved in /tmp/cola/MY_STO2/1/checkpoint-500/special_tokens_map.json
100% 804/804 [10:12<00:00,  1.69it/s][INFO|trainer.py:1129] 2021-05-08 02:11:56,495 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:12<00:00,  1.31it/s]
[INFO|trainer.py:1558] 2021-05-08 02:11:56,696 >> Saving model checkpoint to /tmp/cola/MY_STO2/1/
[INFO|configuration_utils.py:314] 2021-05-08 02:11:56,697 >> Configuration saved in /tmp/cola/MY_STO2/1/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:11:58,299 >> Model weights saved in /tmp/cola/MY_STO2/1/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:11:58,299 >> tokenizer config file saved in /tmp/cola/MY_STO2/1/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:11:58,300 >> Special tokens file saved in /tmp/cola/MY_STO2/1/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:11:58,330 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   train_runtime              = 612.9228
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:11:58,330 >>   train_samples_per_second   =    1.312
[INFO|trainer.py:483] 2021-05-08 02:11:58,433 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 02:11:58,435 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 02:11:58,435 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 02:11:58,435 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.88it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:12:07,434 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,434 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,434 >>   eval_loss                 =  0.5011
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,435 >>   eval_matthews_correlation =  0.5829
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,435 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,435 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,435 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,435 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,435 >>   eval_runtime              =  8.8927
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,435 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:12:07,435 >>   eval_samples_per_second   = 117.287


cola MY_STO2_2

env: POOL_NAME=MY_STO2
2021-05-08 02:12:10.202294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 02:12:11,713 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:12:11,714 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO2",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 02:12:11,816 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:12:11,817 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:12:11,893 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:12:11,893 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:12:11,893 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:12:11,893 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:12:11,893 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 02:12:11,936 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 02:12:15,342 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 02:12:15,342 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 02:12:18,415 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:483] 2021-05-08 02:12:18,416 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:946] 2021-05-08 02:12:18,596 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 02:12:18,596 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 02:12:18,596 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 02:12:18,596 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 02:12:18,596 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 02:12:18,596 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 02:12:18,596 >>   Total optimization steps = 804
 62% 500/804 [06:18<03:49,  1.32it/s][INFO|trainer.py:1558] 2021-05-08 02:18:37,000 >> Saving model checkpoint to /tmp/cola/MY_STO2/2/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 02:18:37,001 >> Configuration saved in /tmp/cola/MY_STO2/2/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:18:38,477 >> Model weights saved in /tmp/cola/MY_STO2/2/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:18:38,479 >> tokenizer config file saved in /tmp/cola/MY_STO2/2/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:18:38,479 >> Special tokens file saved in /tmp/cola/MY_STO2/2/checkpoint-500/special_tokens_map.json
100% 804/804 [10:12<00:00,  1.69it/s][INFO|trainer.py:1129] 2021-05-08 02:22:31,201 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:12<00:00,  1.31it/s]
[INFO|trainer.py:1558] 2021-05-08 02:22:31,410 >> Saving model checkpoint to /tmp/cola/MY_STO2/2/
[INFO|configuration_utils.py:314] 2021-05-08 02:22:31,410 >> Configuration saved in /tmp/cola/MY_STO2/2/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:22:32,966 >> Model weights saved in /tmp/cola/MY_STO2/2/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:22:32,967 >> tokenizer config file saved in /tmp/cola/MY_STO2/2/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:22:32,967 >> Special tokens file saved in /tmp/cola/MY_STO2/2/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:22:33,005 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   train_runtime              = 612.6051
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:33,005 >>   train_samples_per_second   =    1.312
[INFO|trainer.py:483] 2021-05-08 02:22:33,112 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:1775] 2021-05-08 02:22:33,114 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 02:22:33,114 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 02:22:33,114 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.84it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:22:42,225 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,225 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,225 >>   eval_loss                 =  0.5011
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,225 >>   eval_matthews_correlation =  0.5829
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,225 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,225 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,226 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,226 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,226 >>   eval_runtime              =  8.8822
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,226 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:22:42,226 >>   eval_samples_per_second   = 117.426


cola MY_STO2_3

env: POOL_NAME=MY_STO2
2021-05-08 02:22:44.935517: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 02:22:46,401 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:22:46,402 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO2",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 02:22:46,421 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:22:46,422 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:22:46,505 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:22:46,505 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:22:46,505 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:22:46,505 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:22:46,505 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 02:22:46,544 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 02:22:50,113 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 02:22:50,113 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 02:22:53,411 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 02:22:53,412 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 02:22:53,626 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 02:22:53,626 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 02:22:53,626 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 02:22:53,626 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 02:22:53,626 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 02:22:53,626 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 02:22:53,627 >>   Total optimization steps = 804
 62% 500/804 [06:18<03:50,  1.32it/s][INFO|trainer.py:1558] 2021-05-08 02:29:12,327 >> Saving model checkpoint to /tmp/cola//MY_STO2/3/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 02:29:12,328 >> Configuration saved in /tmp/cola//MY_STO2/3/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:29:13,966 >> Model weights saved in /tmp/cola//MY_STO2/3/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:29:13,967 >> tokenizer config file saved in /tmp/cola//MY_STO2/3/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:29:13,967 >> Special tokens file saved in /tmp/cola//MY_STO2/3/checkpoint-500/special_tokens_map.json
100% 804/804 [10:12<00:00,  1.70it/s][INFO|trainer.py:1129] 2021-05-08 02:33:06,317 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:12<00:00,  1.31it/s]
[INFO|trainer.py:1558] 2021-05-08 02:33:06,531 >> Saving model checkpoint to /tmp/cola//MY_STO2/3/
[INFO|configuration_utils.py:314] 2021-05-08 02:33:06,532 >> Configuration saved in /tmp/cola//MY_STO2/3/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:33:08,126 >> Model weights saved in /tmp/cola//MY_STO2/3/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:33:08,126 >> tokenizer config file saved in /tmp/cola//MY_STO2/3/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:33:08,127 >> Special tokens file saved in /tmp/cola//MY_STO2/3/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:33:08,157 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   epoch                      =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   init_mem_cpu_alloc_delta   =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   init_mem_cpu_peaked_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   init_mem_gpu_alloc_delta   =   413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   init_mem_gpu_peaked_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   train_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   train_mem_cpu_peaked_delta =   170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   train_mem_gpu_alloc_delta  =  1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   train_mem_gpu_peaked_delta =  3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   train_runtime              = 612.691
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   train_samples              =    8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:08,157 >>   train_samples_per_second   =   1.312
[INFO|trainer.py:483] 2021-05-08 02:33:08,268 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 02:33:08,270 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 02:33:08,270 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 02:33:08,270 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.90it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:33:17,261 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,261 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,261 >>   eval_loss                 =  0.5011
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,261 >>   eval_matthews_correlation =  0.5829
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,261 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,262 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,262 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,262 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,262 >>   eval_runtime              =  8.8821
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,262 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:33:17,262 >>   eval_samples_per_second   = 117.427


cola MY_STO3

env: POOL_NAME=MY_STO3
2021-05-08 20:39:50.469418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 20:39:52,755 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:39:52,755 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO3",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 20:39:52,958 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:39:52,959 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:39:53,992 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:39:53,992 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:39:53,992 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:39:53,992 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:39:53,992 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 20:39:54,228 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 20:39:57,549 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 20:39:57,549 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 20:40:01,112 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 20:40:01,112 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 20:40:01,289 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 20:40:01,289 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 20:40:01,289 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 20:40:01,289 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 20:40:01,289 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 20:40:01,289 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 20:40:01,289 >>   Total optimization steps = 804
 62% 500/804 [06:11<03:45,  1.35it/s][INFO|trainer.py:1558] 2021-05-08 20:46:12,903 >> Saving model checkpoint to /tmp/cola/MY_STO3/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 20:46:12,904 >> Configuration saved in /tmp/cola/MY_STO3/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:46:14,542 >> Model weights saved in /tmp/cola/MY_STO3/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:46:14,543 >> tokenizer config file saved in /tmp/cola/MY_STO3/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:46:14,544 >> Special tokens file saved in /tmp/cola/MY_STO3/checkpoint-500/special_tokens_map.json
100% 804/804 [10:01<00:00,  1.72it/s][INFO|trainer.py:1129] 2021-05-08 20:50:02,475 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:01<00:00,  1.34it/s]
[INFO|trainer.py:1558] 2021-05-08 20:50:02,689 >> Saving model checkpoint to /tmp/cola/MY_STO3
[INFO|configuration_utils.py:314] 2021-05-08 20:50:02,690 >> Configuration saved in /tmp/cola/MY_STO3/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:50:04,381 >> Model weights saved in /tmp/cola/MY_STO3/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:50:04,382 >> tokenizer config file saved in /tmp/cola/MY_STO3/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:50:04,382 >> Special tokens file saved in /tmp/cola/MY_STO3/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:50:04,417 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   train_runtime              = 601.1859
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:04,417 >>   train_samples_per_second   =    1.337
[INFO|trainer.py:483] 2021-05-08 20:50:04,531 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 20:50:04,532 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 20:50:04,532 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 20:50:04,533 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.23it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:50:13,348 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   eval_loss                 =  0.5302
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   eval_matthews_correlation =  0.5857
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   eval_runtime              =  8.6911
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,348 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:50:13,349 >>   eval_samples_per_second   = 120.007


env: POOL_NAME=MY_STO3
2021-05-08 02:33:19.836163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 02:33:21,518 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:33:21,519 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO3",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 02:33:21,540 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:33:21,540 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:33:21,619 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:33:21,619 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:33:21,619 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:33:21,619 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:33:21,619 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 02:33:21,661 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 02:33:25,119 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 02:33:25,119 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 02:33:28,337 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 02:33:28,338 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 02:33:28,531 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 02:33:28,531 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 02:33:28,531 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 02:33:28,531 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 02:33:28,531 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 02:33:28,532 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 02:33:28,532 >>   Total optimization steps = 804
 62% 500/804 [06:19<03:51,  1.32it/s][INFO|trainer.py:1558] 2021-05-08 02:39:48,029 >> Saving model checkpoint to /tmp/cola//MY_STO3/1/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 02:39:48,030 >> Configuration saved in /tmp/cola//MY_STO3/1/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:39:49,709 >> Model weights saved in /tmp/cola//MY_STO3/1/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:39:49,710 >> tokenizer config file saved in /tmp/cola//MY_STO3/1/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:39:49,710 >> Special tokens file saved in /tmp/cola//MY_STO3/1/checkpoint-500/special_tokens_map.json
100% 804/804 [10:14<00:00,  1.69it/s][INFO|trainer.py:1129] 2021-05-08 02:43:42,948 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:14<00:00,  1.31it/s]
[INFO|trainer.py:1558] 2021-05-08 02:43:43,154 >> Saving model checkpoint to /tmp/cola//MY_STO3/1/
[INFO|configuration_utils.py:314] 2021-05-08 02:43:43,155 >> Configuration saved in /tmp/cola//MY_STO3/1/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:43:44,846 >> Model weights saved in /tmp/cola//MY_STO3/1/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:43:44,846 >> tokenizer config file saved in /tmp/cola//MY_STO3/1/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:43:44,847 >> Special tokens file saved in /tmp/cola//MY_STO3/1/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:43:44,881 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   train_runtime              = 614.4169
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,881 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:44,882 >>   train_samples_per_second   =    1.309
[INFO|trainer.py:483] 2021-05-08 02:43:44,990 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 02:43:44,991 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 02:43:44,991 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 02:43:44,991 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.89it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:43:53,994 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_loss                 =  0.5302
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_matthews_correlation =  0.5857
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_runtime              =  8.8791
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:43:53,994 >>   eval_samples_per_second   = 117.467


cola MY_STO3_2

env: POOL_NAME=MY_STO3
2021-05-08 02:43:56.500795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 02:43:58,045 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:43:58,046 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO3",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 02:43:58,061 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:43:58,061 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:43:58,138 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:43:58,138 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:43:58,138 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:43:58,138 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:43:58,138 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 02:43:58,177 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 02:44:01,590 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 02:44:01,590 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 02:44:04,724 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 02:44:04,724 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 02:44:04,909 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 02:44:04,909 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 02:44:04,909 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 02:44:04,910 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 02:44:04,910 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 02:44:04,910 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 02:44:04,910 >>   Total optimization steps = 804
 62% 500/804 [06:19<03:51,  1.32it/s][INFO|trainer.py:1558] 2021-05-08 02:50:24,424 >> Saving model checkpoint to /tmp/cola//MY_STO3/2/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 02:50:24,425 >> Configuration saved in /tmp/cola//MY_STO3/2/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:50:26,054 >> Model weights saved in /tmp/cola//MY_STO3/2/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:50:26,055 >> tokenizer config file saved in /tmp/cola//MY_STO3/2/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:50:26,056 >> Special tokens file saved in /tmp/cola//MY_STO3/2/checkpoint-500/special_tokens_map.json
100% 804/804 [10:13<00:00,  1.69it/s][INFO|trainer.py:1129] 2021-05-08 02:54:18,855 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:13<00:00,  1.31it/s]
[INFO|trainer.py:1558] 2021-05-08 02:54:19,059 >> Saving model checkpoint to /tmp/cola//MY_STO3/2/
[INFO|configuration_utils.py:314] 2021-05-08 02:54:19,060 >> Configuration saved in /tmp/cola//MY_STO3/2/config.json
[INFO|modeling_utils.py:837] 2021-05-08 02:54:20,622 >> Model weights saved in /tmp/cola//MY_STO3/2/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 02:54:20,623 >> tokenizer config file saved in /tmp/cola//MY_STO3/2/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 02:54:20,623 >> Special tokens file saved in /tmp/cola//MY_STO3/2/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:54:20,653 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,653 >>   epoch                      =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,653 >>   init_mem_cpu_alloc_delta   =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,653 >>   init_mem_cpu_peaked_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,653 >>   init_mem_gpu_alloc_delta   =   413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,653 >>   init_mem_gpu_peaked_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,653 >>   train_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,653 >>   train_mem_cpu_peaked_delta =   170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,653 >>   train_mem_gpu_alloc_delta  =  1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,654 >>   train_mem_gpu_peaked_delta =  3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,654 >>   train_runtime              = 613.945
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,654 >>   train_samples              =    8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:20,654 >>   train_samples_per_second   =    1.31
[INFO|trainer.py:483] 2021-05-08 02:54:20,757 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 02:54:20,759 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 02:54:20,759 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 02:54:20,759 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.90it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 02:54:29,700 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,700 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,700 >>   eval_loss                 =  0.5302
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,700 >>   eval_matthews_correlation =  0.5857
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,700 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,700 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,701 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,701 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,701 >>   eval_runtime              =  8.8382
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,701 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 02:54:29,701 >>   eval_samples_per_second   = 118.011




cola MY_ATTEN

env: POOL_NAME=MY_ATTEN
2021-05-08 20:08:35.633197: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 20:08:37,926 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:08:37,927 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_ATTEN",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 20:08:38,136 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:08:38,136 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:08:39,193 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:08:39,193 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:08:39,193 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:08:39,193 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:08:39,193 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 20:08:39,426 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 20:08:42,744 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 20:08:42,744 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 20:08:46,486 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 20:08:46,487 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 20:08:46,659 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 20:08:46,659 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 20:08:46,659 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 20:08:46,659 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 20:08:46,660 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 20:08:46,660 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 20:08:46,660 >>   Total optimization steps = 804
 62% 500/804 [06:12<03:46,  1.34it/s][INFO|trainer.py:1558] 2021-05-08 20:14:59,352 >> Saving model checkpoint to /tmp/cola/MY_ATTEN/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 20:14:59,353 >> Configuration saved in /tmp/cola/MY_ATTEN/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:15:00,973 >> Model weights saved in /tmp/cola/MY_ATTEN/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:15:00,974 >> tokenizer config file saved in /tmp/cola/MY_ATTEN/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:15:00,974 >> Special tokens file saved in /tmp/cola/MY_ATTEN/checkpoint-500/special_tokens_map.json
100% 804/804 [10:03<00:00,  1.72it/s][INFO|trainer.py:1129] 2021-05-08 20:18:49,816 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:03<00:00,  1.33it/s]
[INFO|trainer.py:1558] 2021-05-08 20:18:50,021 >> Saving model checkpoint to /tmp/cola/MY_ATTEN
[INFO|configuration_utils.py:314] 2021-05-08 20:18:50,022 >> Configuration saved in /tmp/cola/MY_ATTEN/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:18:51,639 >> Model weights saved in /tmp/cola/MY_ATTEN/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:18:51,640 >> tokenizer config file saved in /tmp/cola/MY_ATTEN/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:18:51,640 >> Special tokens file saved in /tmp/cola/MY_ATTEN/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:18:51,675 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   train_mem_gpu_peaked_delta =   3413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   train_runtime              = 603.1562
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:18:51,675 >>   train_samples_per_second   =    1.333
[INFO|trainer.py:483] 2021-05-08 20:18:51,775 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 20:18:51,777 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 20:18:51,777 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 20:18:51,777 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.94it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 20:19:00,658 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,658 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,658 >>   eval_loss                 =  0.5572
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,658 >>   eval_matthews_correlation =  0.5882
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,658 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,658 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,658 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,658 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,659 >>   eval_runtime              =  8.7727
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,659 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 20:19:00,659 >>   eval_samples_per_second   = 118.892

[INFO|trainer.py:1558] 2021-05-07 16:39:53,066 >> Saving model checkpoint to /tmp//
[INFO|configuration_utils.py:314] 2021-05-07 16:39:53,066 >> Configuration saved in /tmp//config.json
[INFO|modeling_utils.py:837] 2021-05-07 16:39:54,712 >> Model weights saved in /tmp//pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-07 16:39:54,713 >> tokenizer config file saved in /tmp//tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-07 16:39:54,713 >> Special tokens file saved in /tmp//special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-07 16:39:54,747 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,747 >>   train_mem_gpu_peaked_delta =   3413MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,748 >>   train_runtime              = 629.3985
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,748 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:39:54,748 >>   train_samples_per_second   =    1.277
[INFO|trainer.py:483] 2021-05-07 16:39:54,847 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:1775] 2021-05-07 16:39:54,849 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-07 16:39:54,849 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-07 16:39:54,849 >>   Batch size = 8
100% 131/131 [00:09<00:00, 14.20it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-07 16:40:04,201 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,201 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,201 >>   eval_loss                 =  0.5572
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,201 >>   eval_matthews_correlation =  0.5882
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,201 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,202 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,202 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,202 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,202 >>   eval_runtime              =  9.2423
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,202 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-07 16:40:04,202 >>   eval_samples_per_second   = 112.851


cola MY_STO3_3

env: POOL_NAME=MY_STO3
2021-05-08 02:54:32.181286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 02:54:33,726 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:54:33,727 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO3",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 02:54:33,744 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 02:54:33,745 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:54:33,825 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:54:33,825 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:54:33,825 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:54:33,825 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 02:54:33,825 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 02:54:33,867 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 02:54:37,277 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 02:54:37,278 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 02:54:40,405 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 02:54:40,405 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 02:54:40,595 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 02:54:40,595 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 02:54:40,595 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 02:54:40,595 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 02:54:40,596 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 02:54:40,596 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 02:54:40,596 >>   Total optimization steps = 804
 62% 500/804 [06:19<03:50,  1.32it/s][INFO|trainer.py:1558] 2021-05-08 03:00:59,933 >> Saving model checkpoint to /tmp/cola//MY_STO3/3/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 03:00:59,934 >> Configuration saved in /tmp/cola//MY_STO3/3/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 03:01:01,608 >> Model weights saved in /tmp/cola//MY_STO3/3/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 03:01:01,609 >> tokenizer config file saved in /tmp/cola//MY_STO3/3/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 03:01:01,609 >> Special tokens file saved in /tmp/cola//MY_STO3/3/checkpoint-500/special_tokens_map.json
100% 804/804 [10:13<00:00,  1.70it/s][INFO|trainer.py:1129] 2021-05-08 03:04:54,474 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:13<00:00,  1.31it/s]
[INFO|trainer.py:1558] 2021-05-08 03:04:54,679 >> Saving model checkpoint to /tmp/cola//MY_STO3/3/
[INFO|configuration_utils.py:314] 2021-05-08 03:04:54,680 >> Configuration saved in /tmp/cola//MY_STO3/3/config.json
[INFO|modeling_utils.py:837] 2021-05-08 03:04:56,217 >> Model weights saved in /tmp/cola//MY_STO3/3/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 03:04:56,218 >> tokenizer config file saved in /tmp/cola//MY_STO3/3/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 03:04:56,218 >> Special tokens file saved in /tmp/cola//MY_STO3/3/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 03:04:56,248 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,248 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,248 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,248 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   train_runtime              = 613.8784
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:04:56,249 >>   train_samples_per_second   =     1.31
[INFO|trainer.py:483] 2021-05-08 03:04:56,351 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 03:04:56,353 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 03:04:56,353 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 03:04:56,353 >>   Batch size = 8
100% 131/131 [00:08<00:00, 14.94it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 03:05:05,308 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_loss                 =  0.5302
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_matthews_correlation =  0.5857
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_runtime              =  8.8526
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 03:05:05,308 >>   eval_samples_per_second   = 117.818

MY_STO4

env: POOL_NAME=MY_STO4
2021-05-08 20:50:16.104917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 20:50:18,499 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:50:18,500 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO4",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 20:50:18,705 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 20:50:18,705 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:50:19,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:50:19,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:50:19,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:50:19,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 20:50:19,758 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 20:50:19,988 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 20:50:23,357 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 20:50:23,357 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 20:50:27,143 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 20:50:27,144 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 20:50:27,365 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 20:50:27,365 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 20:50:27,365 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 20:50:27,365 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 20:50:27,365 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 20:50:27,365 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 20:50:27,365 >>   Total optimization steps = 804
 62% 500/804 [06:10<03:45,  1.35it/s][INFO|trainer.py:1558] 2021-05-08 20:56:38,164 >> Saving model checkpoint to /tmp/cola/MY_STO4/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 20:56:38,165 >> Configuration saved in /tmp/cola/MY_STO4/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 20:56:39,805 >> Model weights saved in /tmp/cola/MY_STO4/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 20:56:39,806 >> tokenizer config file saved in /tmp/cola/MY_STO4/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 20:56:39,806 >> Special tokens file saved in /tmp/cola/MY_STO4/checkpoint-500/special_tokens_map.json
100% 804/804 [09:59<00:00,  1.73it/s][INFO|trainer.py:1129] 2021-05-08 21:00:27,032 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [09:59<00:00,  1.34it/s]
[INFO|trainer.py:1558] 2021-05-08 21:00:27,243 >> Saving model checkpoint to /tmp/cola/MY_STO4
[INFO|configuration_utils.py:314] 2021-05-08 21:00:27,244 >> Configuration saved in /tmp/cola/MY_STO4/config.json
[INFO|modeling_utils.py:837] 2021-05-08 21:00:28,834 >> Model weights saved in /tmp/cola/MY_STO4/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 21:00:28,834 >> tokenizer config file saved in /tmp/cola/MY_STO4/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 21:00:28,834 >> Special tokens file saved in /tmp/cola/MY_STO4/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 21:00:28,867 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   train_runtime              = 599.6671
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:28,868 >>   train_samples_per_second   =    1.341
[INFO|trainer.py:483] 2021-05-08 21:00:28,970 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 21:00:28,972 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 21:00:28,972 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 21:00:28,972 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.15it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 21:00:37,817 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,817 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,817 >>   eval_loss                 =  0.4937
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,817 >>   eval_matthews_correlation =  0.6007
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,817 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,817 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,818 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,818 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,818 >>   eval_runtime              =  8.7343
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,818 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 21:00:37,818 >>   eval_samples_per_second   = 119.414


MY_STO6

env: POOL_NAME=MY_STO6
2021-05-08 21:59:06.120110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 21:59:08,123 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 21:59:08,123 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO6",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 21:59:08,334 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 21:59:08,334 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 21:59:09,511 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 21:59:09,511 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 21:59:09,511 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 21:59:09,511 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 21:59:09,511 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 21:59:09,740 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 21:59:13,111 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 21:59:13,112 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 21:59:16,714 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:483] 2021-05-08 21:59:16,715 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:946] 2021-05-08 21:59:16,939 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 21:59:16,939 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 21:59:16,939 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 21:59:16,939 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 21:59:16,939 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 21:59:16,939 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 21:59:16,939 >>   Total optimization steps = 804
 62% 500/804 [06:11<03:46,  1.34it/s][INFO|trainer.py:1558] 2021-05-08 22:05:28,192 >> Saving model checkpoint to /tmp/cola/MY_STO6/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 22:05:28,194 >> Configuration saved in /tmp/cola/MY_STO6/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 22:05:29,972 >> Model weights saved in /tmp/cola/MY_STO6/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 22:05:29,973 >> tokenizer config file saved in /tmp/cola/MY_STO6/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 22:05:29,974 >> Special tokens file saved in /tmp/cola/MY_STO6/checkpoint-500/special_tokens_map.json
100% 804/804 [10:01<00:00,  1.72it/s][INFO|trainer.py:1129] 2021-05-08 22:09:18,160 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:01<00:00,  1.34it/s]
[INFO|trainer.py:1558] 2021-05-08 22:09:18,379 >> Saving model checkpoint to /tmp/cola/MY_STO6
[INFO|configuration_utils.py:314] 2021-05-08 22:09:18,380 >> Configuration saved in /tmp/cola/MY_STO6/config.json
[INFO|modeling_utils.py:837] 2021-05-08 22:09:20,018 >> Model weights saved in /tmp/cola/MY_STO6/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 22:09:20,019 >> tokenizer config file saved in /tmp/cola/MY_STO6/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 22:09:20,019 >> Special tokens file saved in /tmp/cola/MY_STO6/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 22:09:20,053 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   train_mem_cpu_alloc_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   train_mem_cpu_peaked_delta =    170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   train_mem_gpu_alloc_delta  =   1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   train_runtime              = 601.2216
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:20,053 >>   train_samples_per_second   =    1.337
[INFO|trainer.py:483] 2021-05-08 22:09:20,170 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: idx, sentence.
[INFO|trainer.py:1775] 2021-05-08 22:09:20,172 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 22:09:20,172 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 22:09:20,172 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.05it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 22:09:29,085 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,085 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,085 >>   eval_loss                 =  0.4936
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,086 >>   eval_matthews_correlation =  0.6007
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,086 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,086 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,086 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,086 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,086 >>   eval_runtime              =  8.7947
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,086 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:09:29,086 >>   eval_samples_per_second   = 118.594




MY_STO7

env: POOL_NAME=MY_STO7
2021-05-08 22:09:32.185452: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 22:09:34,551 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 22:09:34,551 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO7",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 22:09:34,757 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 22:09:34,757 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:09:35,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:09:35,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:09:35,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:09:35,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:09:35,945 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 22:09:36,175 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 22:09:39,529 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 22:09:39,530 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 22:09:43,309 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 22:09:43,309 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:946] 2021-05-08 22:09:43,534 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 22:09:43,534 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 22:09:43,534 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 22:09:43,534 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 22:09:43,535 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 22:09:43,535 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 22:09:43,535 >>   Total optimization steps = 804
 62% 500/804 [06:11<03:45,  1.35it/s][INFO|trainer.py:1558] 2021-05-08 22:15:54,848 >> Saving model checkpoint to /tmp/cola/MY_STO7/checkpoint-500
[INFO|configuration_utils.py:314] 2021-05-08 22:15:54,849 >> Configuration saved in /tmp/cola/MY_STO7/checkpoint-500/config.json
[INFO|modeling_utils.py:837] 2021-05-08 22:15:56,795 >> Model weights saved in /tmp/cola/MY_STO7/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 22:15:56,796 >> tokenizer config file saved in /tmp/cola/MY_STO7/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 22:15:56,796 >> Special tokens file saved in /tmp/cola/MY_STO7/checkpoint-500/special_tokens_map.json
100% 804/804 [10:01<00:00,  1.73it/s][INFO|trainer.py:1129] 2021-05-08 22:19:44,948 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [10:01<00:00,  1.34it/s]
[INFO|trainer.py:1558] 2021-05-08 22:19:45,166 >> Saving model checkpoint to /tmp/cola/MY_STO7
[INFO|configuration_utils.py:314] 2021-05-08 22:19:45,167 >> Configuration saved in /tmp/cola/MY_STO7/config.json
[INFO|modeling_utils.py:837] 2021-05-08 22:19:47,063 >> Model weights saved in /tmp/cola/MY_STO7/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 22:19:47,063 >> tokenizer config file saved in /tmp/cola/MY_STO7/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 22:19:47,064 >> Special tokens file saved in /tmp/cola/MY_STO7/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 22:19:47,103 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,103 >>   epoch                      =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,103 >>   init_mem_cpu_alloc_delta   =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,103 >>   init_mem_cpu_peaked_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,103 >>   init_mem_gpu_alloc_delta   =   413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,103 >>   init_mem_gpu_peaked_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,104 >>   train_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,104 >>   train_mem_cpu_peaked_delta =   170MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,104 >>   train_mem_gpu_alloc_delta  =  1298MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,104 >>   train_mem_gpu_peaked_delta =  3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,104 >>   train_runtime              = 601.413
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,104 >>   train_samples              =    8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:47,104 >>   train_samples_per_second   =   1.337
[INFO|trainer.py:483] 2021-05-08 22:19:47,219 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 22:19:47,221 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 22:19:47,221 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 22:19:47,221 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.17it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 22:19:56,063 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_loss                 =  0.4921
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_matthews_correlation =  0.5959
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_runtime              =   8.725
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:19:56,063 >>   eval_samples_per_second   = 119.541



MY_STO8
env: POOL_NAME=MY_STO8
2021-05-08 22:19:58.851044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[INFO|configuration_utils.py:463] 2021-05-08 22:20:01,212 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 22:20:01,212 >> Model config MyBertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO8",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:463] 2021-05-08 22:20:01,433 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:499] 2021-05-08 22:20:01,433 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:20:02,480 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:20:02,480 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:20:02,480 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:20:02,480 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1702] 2021-05-08 22:20:02,480 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|modeling_utils.py:1051] 2021-05-08 22:20:02,711 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1159] 2021-05-08 22:20:06,087 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing MyBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1170] 2021-05-08 22:20:06,087 >> Some weights of MyBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.pooler.fc.weight', 'bert.pooler.fc.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:483] 2021-05-08 22:20:09,923 >> The following columns in the training set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:483] 2021-05-08 22:20:09,924 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:860] 2021-05-08 22:20:10,147 >> Loading model from /tmp/cola/MY_STO8/checkpoint-500).
[INFO|configuration_utils.py:461] 2021-05-08 22:20:10,149 >> loading configuration file /tmp/cola/MY_STO8/checkpoint-500/config.json
[INFO|configuration_utils.py:499] 2021-05-08 22:20:10,150 >> Model config BertConfig {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "MyBertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooling_layer_type": "MY_STO8",
  "position_embedding_type": "absolute",
  "transformers_version": "4.4.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|modeling_utils.py:1049] 2021-05-08 22:20:10,150 >> loading weights file /tmp/cola/MY_STO8/checkpoint-500/pytorch_model.bin
[INFO|modeling_utils.py:1167] 2021-05-08 22:20:24,955 >> All model checkpoint weights were used when initializing MyBertForSequenceClassification.

[INFO|modeling_utils.py:1176] 2021-05-08 22:20:24,955 >> All the weights of MyBertForSequenceClassification were initialized from the model checkpoint at /tmp/cola/MY_STO8/checkpoint-500.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MyBertForSequenceClassification for predictions without further training.
[INFO|trainer.py:946] 2021-05-08 22:20:49,147 >> ***** Running training *****
[INFO|trainer.py:947] 2021-05-08 22:20:49,148 >>   Num examples = 8551
[INFO|trainer.py:948] 2021-05-08 22:20:49,148 >>   Num Epochs = 3
[INFO|trainer.py:949] 2021-05-08 22:20:49,148 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:950] 2021-05-08 22:20:49,148 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:951] 2021-05-08 22:20:49,148 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:952] 2021-05-08 22:20:49,148 >>   Total optimization steps = 804
[INFO|trainer.py:971] 2021-05-08 22:20:49,149 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:972] 2021-05-08 22:20:49,149 >>   Continuing training from epoch 1
[INFO|trainer.py:973] 2021-05-08 22:20:49,149 >>   Continuing training from global step 500
[INFO|trainer.py:976] 2021-05-08 22:20:49,149 >>   Will skip the first 1 epochs then the first 232 batches in the first epoch.
100% 804/804 [03:41<00:00,  1.72it/s][INFO|trainer.py:1129] 2021-05-08 22:24:30,692 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


100% 804/804 [03:41<00:00,  3.63it/s]
[INFO|trainer.py:1558] 2021-05-08 22:24:30,919 >> Saving model checkpoint to /tmp/cola/MY_STO8
[INFO|configuration_utils.py:314] 2021-05-08 22:24:30,920 >> Configuration saved in /tmp/cola/MY_STO8/config.json
[INFO|modeling_utils.py:837] 2021-05-08 22:24:32,372 >> Model weights saved in /tmp/cola/MY_STO8/pytorch_model.bin
[INFO|tokenization_utils_base.py:1896] 2021-05-08 22:24:32,373 >> tokenizer config file saved in /tmp/cola/MY_STO8/tokenizer_config.json
[INFO|tokenization_utils_base.py:1902] 2021-05-08 22:24:32,373 >> Special tokens file saved in /tmp/cola/MY_STO8/special_tokens_map.json
[INFO|trainer_pt_utils.py:656] 2021-05-08 22:24:32,405 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   epoch                      =      3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   init_mem_cpu_alloc_delta   =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   init_mem_cpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   init_mem_gpu_alloc_delta   =    413MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   init_mem_gpu_peaked_delta  =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   train_mem_cpu_alloc_delta  =      1MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   train_mem_cpu_peaked_delta =      0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   train_mem_gpu_alloc_delta  =   1682MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   train_mem_gpu_peaked_delta =   3395MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   train_runtime              = 221.5445
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   train_samples              =     8551
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:32,405 >>   train_samples_per_second   =    3.629
[INFO|trainer.py:483] 2021-05-08 22:24:32,527 >> The following columns in the evaluation set  don't have a corresponding argument in `MyBertForSequenceClassification.forward` and have been ignored: sentence, idx.
[INFO|trainer.py:1775] 2021-05-08 22:24:32,529 >> ***** Running Evaluation *****
[INFO|trainer.py:1776] 2021-05-08 22:24:32,529 >>   Num examples = 1043
[INFO|trainer.py:1777] 2021-05-08 22:24:32,529 >>   Batch size = 8
100% 131/131 [00:08<00:00, 15.02it/s]
[INFO|trainer_pt_utils.py:656] 2021-05-08 22:24:41,419 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   epoch                     =     3.0
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_loss                 =   0.487
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_matthews_correlation =  0.5941
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_mem_cpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_mem_cpu_peaked_delta =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_mem_gpu_alloc_delta  =     0MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_mem_gpu_peaked_delta =    33MB
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_runtime              =  8.7688
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_samples              =    1043
[INFO|trainer_pt_utils.py:661] 2021-05-08 22:24:41,419 >>   eval_samples_per_second   = 118.944